{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Review Generation Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- This is a self-contained notebook but we also demo the sampling of prompts from the original Amazon dataset so please upload the file from path 'amazon_reviews/test/amazon_reviews.txt' with the notebook to colab\n",
    "- Our best model is provided (the category conditioned one), in the training folder, please upload this model to colab alongside this notebook to load it\n",
    "- We have marked below points where the marker can enter their own inputs into the model to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'id' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",                    \n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "MODEL = 'distilgpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2:\n",
    "    def __init__(self, model_path=None, full_model=False, special_tokens=None) -> None:\n",
    "        self.tokenizer = self.get_tokenizer(special_tokens)\n",
    "        self.model = self.get_model(self.tokenizer, special_tokens=special_tokens, load_model_path=model_path, full_model=full_model)\n",
    "        \n",
    "    def get_tokenizer(self, special_tokens=None):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
    "\n",
    "        if special_tokens:\n",
    "            tokenizer.add_special_tokens(special_tokens)\n",
    "        return tokenizer\n",
    "    \n",
    "    def get_model(self, tokenizer, special_tokens=None, load_model_path=None, full_model=False):\n",
    "        if full_model:\n",
    "            model = AutoModelForCausalLM.from_pretrained(load_model_path)\n",
    "            model.cuda()\n",
    "            return model \n",
    "        \n",
    "        if special_tokens:\n",
    "            config = AutoConfig.from_pretrained(MODEL, \n",
    "                                                bos_token_id=tokenizer.bos_token_id,\n",
    "                                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                                sep_token_id=tokenizer.sep_token_id,\n",
    "                                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                                output_hidden_states=False)\n",
    "        else: \n",
    "            config = AutoConfig.from_pretrained(MODEL,                                     \n",
    "                                                pad_token_id=tokenizer.eos_token_id,\n",
    "                                                output_hidden_states=False)    \n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL, config=config)\n",
    "\n",
    "        if special_tokens:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        if load_model_path:\n",
    "            model.load_state_dict(torch.load(load_model_path))#map_location=torch.device('cpu')))\n",
    "\n",
    "        model.to(torch.device('cuda:0'))\n",
    "        return model\n",
    "    \n",
    "    def generate_text(self, prompt, category, print_output=True, **kwargs):\n",
    "        generated_outputs = []\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        tokenized_prompt = self.tokenizer.encode(prompt, return_tensors='pt').to('cuda:0')\n",
    "        \n",
    "        # Language modelling\n",
    "        output = self.model.generate(tokenized_prompt, **kwargs)\n",
    "        \n",
    "        for i, o in enumerate(output):\n",
    "            gen_txt = self.tokenizer.decode(o, skip_special_tokens=True)\n",
    "            gen_txt = gen_txt[len(category):]\n",
    "            truncated_txt = gen_txt.split('.')\n",
    "            truncated_txt = '.'.join(truncated_txt[:-1]) + '.'\n",
    "            generated_outputs.append(truncated_txt)\n",
    "            \n",
    "            if print_output:\n",
    "                print(truncated_txt + '\\n')\n",
    "                \n",
    "        return generated_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_start_amazon(df, length=5):\n",
    "    sample = df.sample(n=1)\n",
    "    title, category, text = list(sample['REVIEW_TITLE'])[0], list(sample['PRODUCT_CATEGORY'])[0], list(sample['REVIEW_TEXT'])[0]\n",
    "    sample = str(text).split(' ')\n",
    "    return ' '.join(sample[:length]), title, category, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please ensure the amazon_reviews.txt file stated at the top of this notebook is in the working dir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load our test-data that we will be sampling categories and prompts from\n",
    "data = pd.read_csv('amazon_reviews.txt')\n",
    "data.loc[data['LABEL'] == '__label2__', 'LABEL'] = 0\n",
    "data.loc[data['LABEL'] == '__label1__', 'LABEL'] = 1\n",
    "data_amazon = data.get(data['LABEL'] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure the pytorch_model.bin file is in the working directory to be loaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the category model\n",
    "model_path = 'pytorch_model.bin'\n",
    "model = GPT2(model_path=model_path, full_model=False, special_tokens=SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the available categories\n",
    "categories = ['Apparel', 'Automotive', 'Baby', 'Beauty', 'Books', 'Camera', 'Electronics', 'Furniture', 'Grocery', 'Health & Personal Care', 'Home', 'Home Entertainment', 'Home Improvement', 'Jewelry', 'Kitchen', 'Lawn and Garden', 'Luggage', 'Musical Instruments', 'Office Products', 'Outdoors', 'PC', 'Pet Products', 'Shoes', 'Sports', 'Tools', 'Toys', 'Video DVD', 'Video Games', 'Watches', 'Wireless']\n",
    "start_words = {'A', 'After', 'All', 'Any', 'Apart', 'Arrived', 'As', 'At', 'Attended', 'Avoid', 'Awesome', 'Be', 'Beautiful', 'Before', 'Booked', 'Check', \"Didn't\", 'Despite', 'Do', \"Don't\", 'Elegant', 'Even', 'Excellent', 'First', 'Firstly', 'For', 'From', 'Generally', 'Going', 'Good', 'Got', 'Great', 'Guys', 'Had', 'Have', 'Having', 'Here', 'How', 'I', \"I'd\", \"I'll\", \"I'm\", \"I've\", 'If', 'In', 'It', \"It's\", 'Just', 'Let', 'Me', 'My', 'Nice', 'No', 'Not', 'Often', 'Ok,', 'On', 'Other', 'Our', 'Overall', 'Recently', 'Rude,', 'Seriously', 'Simply', 'Sometimes', 'The', 'They', 'This', 'Used', 'Very', 'Was', 'We', \"We've\", 'Well', 'Went', 'What', \"What's\", 'When', 'While'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = np.random.choice(start_words)\n",
    "cat = np.random.choice(categories)\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + cat + SPECIAL_TOKENS['sep_token'] + prompt\n",
    "outputs = model.generate_text(prompt, cat, print_output=True, do_sample=True, max_length=200, num_beams=5, repetition_penalty=5.0, early_stopping=True, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from Amazon human set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, title, cat, original = sample_start_amazon(data_amazon, length=np.random.randint(4, 8))\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + cat + SPECIAL_TOKENS['sep_token'] + prompt\n",
    "outputs = model.generate_text(prompt, cat, print_output=True, do_sample=True, max_length=70, num_beams=5, repetition_penalty=5.0, early_stopping=True, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free input section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'PUT YOUR PROMPT HERE'\n",
    "cat = 'PLEASE SELECT ONLY CATEGORIES AVAILABLE IN THE CATEGORIES LIST ABOVE'\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + cat + SPECIAL_TOKENS['sep_token'] + prompt\n",
    "outputs = model.generate_text(prompt, cat, print_output=True, do_sample=True, max_length=70, num_beams=5, repetition_penalty=5.0, early_stopping=True, num_return_sequences=3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13872f827bf2face4951d508a343680c0c465a86f8c76a51d647b255bdadb53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
