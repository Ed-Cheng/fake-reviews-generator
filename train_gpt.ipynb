{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "#from models.gpt import GPT2\n",
    "from utils.loader import DataLoader\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset_obj(dataframe, columns):\n",
    "    dataset = datasets.Dataset.from_pandas(dataframe[columns])\n",
    "    dataset = dataset.remove_columns('__index_level_0__')\n",
    "    dataset = dataset.rename_column('LABEL', 'labels')\n",
    "    dataset = dataset.rename_column('REVIEW_TEXT', 'text')\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(inputs):\n",
    "    return tokenizer(inputs['text'], padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.2\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "# Load datasets\n",
    "loader = DataLoader()\n",
    "#dec_data = loader.load_amazon(deceptive=True)\n",
    "truth_data = loader.load_amazon()\n",
    "truth_data = truth_data.sample(frac=1)\n",
    "truth_data_val = truth_data.iloc[:int(0.2*len(truth_data))]\n",
    "truth_data_train = truth_data.iloc[int(0.2*len(truth_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and convert to Dataset objects\n",
    "#dataset_dec = df_to_dataset_obj(dec_data, ['LABEL', 'REVIEW_TEXT'])\n",
    "dataset_truth_val = df_to_dataset_obj(truth_data_val, ['LABEL', 'REVIEW_TEXT'])\n",
    "dataset_truth_train = df_to_dataset_obj(truth_data_train, ['LABEL', 'REVIEW_TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:02<00:00,  3.84ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenized_dec = dataset_dec.map(tokenize_data(tokenizer=), batched=True)\n",
    "tokenized_val = dataset_truth_val.map(tokenize_data, batched=True, remove_columns=['labels', 'text'])\n",
    "tokenized_train = dataset_truth_train.map(tokenize_data, batched=True, remove_columns=['labels', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_truth_val\n",
    "del dataset_truth_train\n",
    "del truth_data_val\n",
    "del truth_data_train\n",
    "del truth_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set padding token and use mlm to use the inputs as the labels shifted to right by one\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 336M/336M [00:34<00:00, 10.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"sample_data\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=4\n",
    "    \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3939\n",
      "  0%|          | 1/3939 [01:03<69:05:48, 63.17s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13872f827bf2face4951d508a343680c0c465a86f8c76a51d647b255bdadb53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
