{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine tunning model: https://huggingface.co/docs/transformers/training\n",
    "\n",
    "testing model: https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import DataLoader\n",
    "import datasets\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "loader = DataLoader()\n",
    "train_data = loader.load_amazon(deceptive=False, all=True, test_mode=False)\n",
    "test_data = loader.load_amazon(deceptive=False, all=True, test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beauty' 'Office Products' 'Grocery']\n"
     ]
    }
   ],
   "source": [
    "# choose 3 categories to train with\n",
    "unique_category = train_data['PRODUCT_CATEGORY'].unique()\n",
    "random.seed(22)\n",
    "print(unique_category[random.sample(range(30), 3)])\n",
    "# gogo = train_data[train_data['PRODUCT_CATEGORY'].isin(['Jewelry' 'Outdoors'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_2_dataset(panda_data, VERIFIED_PURCHASE, PRODUCT_CATEGORY=False):\n",
    "    if PRODUCT_CATEGORY != False:\n",
    "        panda_data = panda_data[panda_data['PRODUCT_CATEGORY'].isin(PRODUCT_CATEGORY)]\n",
    "\n",
    "    panda_data = panda_data[panda_data['VERIFIED_PURCHASE'].isin(VERIFIED_PURCHASE)]\n",
    "\n",
    "    dataset_data = datasets.Dataset.from_pandas(panda_data)\n",
    "    remove_columns = ['DOC_ID',\n",
    "                    'RATING',\n",
    "                    'VERIFIED_PURCHASE',\n",
    "                    'PRODUCT_CATEGORY',\n",
    "                    'PRODUCT_ID',\n",
    "                    'PRODUCT_TITLE',\n",
    "                    'REVIEW_TITLE',\n",
    "                    '__index_level_0__']\n",
    "    for col in remove_columns:\n",
    "        dataset_data = dataset_data.remove_columns(col) \n",
    "    dataset_data = dataset_data.rename_column('LABEL', 'labels')\n",
    "    dataset_data = dataset_data.rename_column('REVIEW_TEXT', 'text')\n",
    "\n",
    "    return dataset_data\n",
    "\n",
    "verified = ['Y']\n",
    "category = ['Grocery']\n",
    "df_train = df_2_dataset(train_data, verified, category)\n",
    "df_test = df_2_dataset(test_data, verified, category)\n",
    "df_full_test = df_2_dataset(test_data, verified, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'text'],\n",
      "    num_rows: 233\n",
      "}) Dataset({\n",
      "    features: ['labels', 'text'],\n",
      "    num_rows: 78\n",
      "}) Dataset({\n",
      "    features: ['labels', 'text'],\n",
      "    num_rows: 2866\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(df_train, df_test, df_full_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.78ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.88ba/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  7.05ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# train and val sets\n",
    "tokenized_train = df_train.map(tokenize_function, batched=True)\n",
    "tokenized_train = tokenized_train.train_test_split(test_size=0.1)\n",
    "\n",
    "# test set\n",
    "tokenized_test = df_test.map(tokenize_function, batched=True)\n",
    "tokenized_full_test = df_full_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                  # evaluation_strategy=\"steps\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  num_train_epochs=5,\n",
    "                                  save_total_limit = 1,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_strategy = \"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'text', 'token_type_ids'],\n",
      "    num_rows: 480\n",
      "}) Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'text', 'token_type_ids'],\n",
      "    num_rows: 54\n",
      "}) Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'text', 'token_type_ids'],\n",
      "    num_rows: 166\n",
      "}) Dataset({\n",
      "    features: ['attention_mask', 'input_ids', 'labels', 'text', 'token_type_ids'],\n",
      "    num_rows: 5250\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# reduce data size for debugging\n",
    "# with 100 training data we get ~60%\n",
    "testing = False\n",
    "\n",
    "if testing:\n",
    "  train_dataset = tokenized_train['train'].shuffle(seed=42).select(range(200))\n",
    "  val_dataset = tokenized_train['test'].shuffle(seed=42).select(range(100))\n",
    "  test_dataset = tokenized_test.shuffle(seed=42).select(range(200))\n",
    "  test_full_dataset = tokenized_full_test.shuffle(seed=42).select(range(200))\n",
    "else:\n",
    "  train_dataset = tokenized_train['train']\n",
    "  val_dataset = tokenized_train['test']\n",
    "  test_dataset = tokenized_test\n",
    "  test_full_dataset = tokenized_full_test\n",
    "\n",
    "print(train_dataset, val_dataset, test_dataset,tokenized_full_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [test_dataset, test_full_dataset]:\n",
    "    predictions = trainer.predict(d)\n",
    "    print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/Colab Notebooks/2022-03 SNLP group project/tuned_classifier'\n",
    "trainer.save_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=2)\n",
    "\n",
    "test_trainer = Trainer(new_model)\n",
    "\n",
    "ppp = test_trainer.predict(test_dataset)\n",
    "print(ppp.predictions.shape, ppp.label_ids.shape)\n",
    "\n",
    "pd = np.argmax(ppp.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=pd, references=ppp.label_ids)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3afd8f29d5831e8390c3b3041509e615069f2b27f76c79fda71dc34b684b0001"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('MV00')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
