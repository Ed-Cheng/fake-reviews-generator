{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most of this notebook directly come from online source, the code layout is pretty bad, might take a while to understand what the original author is writing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\edton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated2\n"
     ]
    }
   ],
   "source": [
    "from utils.loader import DataLoader\n",
    "loader = DataLoader()\n",
    "loader.testing()\n",
    "gg = loader.load_gold_data(_type = 'dec', neg_polarity=True, pos_polarity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np\n"
     ]
    }
   ],
   "source": [
    "_type = 'dedc'\n",
    "if _type not in DataLoader().types.keys():\n",
    "    print('np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(shuffle = True):\n",
    "    dec_data = loader.load_gold_data(_type = 'dec', neg_polarity=True, pos_polarity=True)\n",
    "    truth_data = loader.load_gold_data(_type = 'truth', neg_polarity=True, pos_polarity=True)\n",
    "\n",
    "    for i in range(len(dec_data)):\n",
    "        rawData.append((dec_data[i], 'fake'))\n",
    "        rawData.append((truth_data[i], 'real'))\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(rawData)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (Text, Label) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        trainData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "    for (Text, Label) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        testData.append((toFeatureVector(preProcess(Text)),Label))\n",
    "# QUESTION 1\n",
    "\n",
    "\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    s=\"\"\n",
    "    if reviewLine[1]==\"__label1__\":\n",
    "        s = \"fake\"\n",
    "    else: \n",
    "        s = \"real\"\n",
    "    return (reviewLine[8], s)\n",
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "# Input: a string of one review\n",
    "table = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens=[]\n",
    "    lemmatized_tokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.translate(table)\n",
    "    for w in text.split(\" \"):\n",
    "\n",
    "        ######################## try without removing ################################################################\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(w.lower()))\n",
    "\n",
    "\n",
    "        # if w not in stop_words:\n",
    "        #     lemmatized_tokens.append(lemmatizer.lemmatize(w.lower()))\n",
    "        filtered_tokens = [' '.join(l) for l in nltk.bigrams(lemmatized_tokens)] + lemmatized_tokens\n",
    "    return filtered_tokens\n",
    "featureDict = {} # A global dictionary of features\n",
    "\n",
    "\n",
    "def toFeatureVector(tokens):\n",
    "    localDict = {}\n",
    "            \n",
    "#Text        \n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in featureDict:\n",
    "            featureDict[token] = 1\n",
    "        else:\n",
    "            featureDict[token] = +1\n",
    "            \n",
    "        if token not in localDict:\n",
    "            localDict[token] = 1\n",
    "        else:\n",
    "            localDict[token] = +1\n",
    "    \n",
    "    return localDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.configs import config\n",
    "from utils.loader import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data = loader.load_amazon(deceptive=False, all=True, test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawAmazon = []\n",
    "for i in range(len(amazon_data)):\n",
    "    if amazon_data['LABEL'][i] == 1:\n",
    "        rawAmazon.append((amazon_data['REVIEW_TEXT'][i], 'fake'))\n",
    "    else:\n",
    "        rawAmazon.append((amazon_data['REVIEW_TEXT'][i], 'real'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I a so happy I bought this. I loved the show but I missed some of them. Thank you for putting it on sale. I hope others would watch the show and get hooked like I did.',\n",
       " 'real')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawAmazon[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAmazon = []\n",
    "for (Text, Label) in rawAmazon:\n",
    "    testAmazon.append((toFeatureVector(preProcess(Text)),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 1600 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 1600 rawData, 1280 trainData, 320 testData\n",
      "Training Samples: \n",
      "1280\n",
      "Features: \n",
      "89083\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "\n",
    "# references to the data files\n",
    "path = os.path.join(config['base_path'], config['amazon_path'], 'train')\n",
    "reviewPath = DataLoader.list_all_txt_files(path)[0]\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "\n",
    "loadData(shuffle = True)\n",
    "\n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "\n",
    "      \n",
    "splitData(0.8)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i stayed at this hotel for a week with my family this hotel is huge so clean has comfy beds foods great staff couldn,t of being any nicer. stayed here for thanksgiving it was fabulous second time in chicago and still want to go back again ... the hotel is within walkin distance to both state street and michicgan avenue.There,s taxis right outside the hotel and the concierge can also ring for mini buses if your travelling with a big crowd.there,s also a cinema 2 mins away opposite the hotel . Perfect Hotel for a great get away ... ',\n",
       "  'real'),\n",
       " (\"The reviews we read were a bit mixed, but I thought it was excellent. I've stayed in The Splendido, Villa D'Este and others and it doesn't compare, but nor should it. It doesn't claim to be in that space. I expected it to be a notch off the Four Seasons and so it proved. A only a small notch too!. Lovely clean and bright room. Superb views, very quiet, comfy bed etc. Excellent service all round and nice staff. For the money and quality it stacked up very well indeed. Couldn't really fault it any way given it was half the cost of the Four Seasons but pretty close in terms of overall experience. We will go there again when in Chicago. Great location too. \",\n",
       "  'real'),\n",
       " ('I stayed here for 5 nights last summer. I booked the reservation on priceline for $75/night which was the cheapest rate I could find anywhere during a convention week. Anyway, when I arrived at 11 pm from the airport, I was told that they had overbooked the hotel and had no room for me. They booked me at a dumpy 2 star motel by the airport. They gave me $20 for a taxi to get there, and it cost me $35. I was late for a meeting the next morning because of the extra travel and packing/unpacking. The Millennium offered no compensation whatsoever for their foul-up. No free drink, no room upgrade, nothing. Since I booked on the internet, I was chopped liver. Beware.',\n",
       "  'real'),\n",
       " (\"The Sofitel Water Tower was a wonderful place to stay for my husband and me for our 20th anniversary weekend trip to Chicago. We got married in the city but then moved away, and it was so romantic to return. We loved how near the Sofitel was to everything - the Magnificent Mile was just outside our door, and we could walk to Navy Pier and other attractions as well. Chicago has changed, but the Sofitel made it feel like home again... except a very plush and elegant version of home! We stayed in a Junior Suite, which had great views of the city, and room service was very attentive when we called for champagne after midnight the day we arrived. Both nights before leaving for supper, we had a cocktail in Le Bar and enjoyed talking about what we'd done during the day, and all the great memories this town has for us. But I have to say we have some new great memories now, thanks to the Sofitel Water Tower!\",\n",
       "  'fake'),\n",
       " ('DO NOT STAY HERE!! My wife and I were visiting family in Chicago and decided to stay at the James. We had stayed at the James in New York happily so we came in with good expectations. What a mistake. Our room looked nothing like the ones pictured on the website. The bathroom was dirty and the bedding and furniture were disheveled. There was also a musty odor to the room. When we called the front desk to complain they were quite rude. We ended up leaving and staying in a different hotel for the duration of our stay.',\n",
       "  'fake')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['real',\n",
       " 'real',\n",
       " 'real',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'real',\n",
       " 'fake',\n",
       " 'fake',\n",
       " 'fake']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rawData[i][1] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    return SklearnClassifier(pipeline).train(trainData)\n",
    "\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "    \n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    cv_results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "        classifier = trainClassifier(dataset[:i]+dataset[foldSize+i:])\n",
    "        y_pred = predictLabels(dataset[i:i+foldSize],classifier)\n",
    "        a = accuracy_score(list(map(lambda d : d[1], dataset[i:i+foldSize])), y_pred)\n",
    "        (p,r,f,_) = precision_recall_fscore_support(list(map(lambda d : d[1], dataset[i:i+foldSize])), y_pred, average ='macro')\n",
    "        #print(a,p,r,f)\n",
    "        cv_results.append((a,p,r,f))\n",
    "    cv_results = (np.mean(np.array(cv_results),axis=0))\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "def MNB_Classifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "    return MNB_classifier.train(trainData)\n",
    "\n",
    "\n",
    "def BernoulliNB_Classifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "    return BernoulliNB_classifier.train(trainData)\n",
    "\n",
    "\n",
    "def LogisticRegression_Classifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    LogisticRegression_classifier = SklearnClassifier(LogisticRegression(max_iter=1000))\n",
    "    return LogisticRegression_classifier.train(trainData)\n",
    "\n",
    "def SGDClassifier_Classifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "    return SGDClassifier_classifier.train(trainData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5250"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testAmazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# run this to change the testData\n",
    "\n",
    "test_Data = testAmazon\n",
    "# test_Data = testData\n",
    "\n",
    "\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "accuracy:  0.5876190476190476\n",
      "Precision:  0.6003989474577709\n",
      "Recall:  0.5876190476190476\n",
      "f1-score:  0.5778410263608078\n"
     ]
    }
   ],
   "source": [
    "#  TEST DATA\n",
    "classifier = trainClassifier(trainData)\n",
    "predictions = predictLabels(test_Data, classifier)\n",
    "true_labels = list(map(lambda d: d[1], test_Data))\n",
    "a = accuracy_score(true_labels, predictions)\n",
    "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "print(\"accuracy: \", a)\n",
    "print(\"Precision: \", p)\n",
    "print(\"Recall: \", a)\n",
    "print(\"f1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "accuracy:  0.5775238095238096\n",
      "Precision:  0.5781200348730149\n",
      "Recall:  0.5775238095238096\n",
      "f1-score:  0.5773940340355219\n"
     ]
    }
   ],
   "source": [
    "#  TEST DATA\n",
    "classifier = MNB_Classifier(trainData)\n",
    "predictions = predictLabels(test_Data, classifier)\n",
    "true_labels = list(map(lambda d: d[1], test_Data))\n",
    "a = accuracy_score(true_labels, predictions)\n",
    "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "print(\"accuracy: \", a)\n",
    "print(\"Precision: \", p)\n",
    "print(\"Recall: \", a)\n",
    "print(\"f1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "accuracy:  0.5601904761904762\n",
      "Precision:  0.6269458724442367\n",
      "Recall:  0.5601904761904762\n",
      "f1-score:  0.4831428174172929\n"
     ]
    }
   ],
   "source": [
    "#  TEST DATA\n",
    "classifier = BernoulliNB_Classifier(trainData)\n",
    "predictions = predictLabels(test_Data, classifier)\n",
    "true_labels = list(map(lambda d: d[1], test_Data))\n",
    "a = accuracy_score(true_labels, predictions)\n",
    "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "print(\"accuracy: \", a)\n",
    "print(\"Precision: \", p)\n",
    "print(\"Recall: \", a)\n",
    "print(\"f1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "accuracy:  0.5939047619047619\n",
      "Precision:  0.6058114224014152\n",
      "Recall:  0.5939047619047619\n",
      "f1-score:  0.5854904396534371\n"
     ]
    }
   ],
   "source": [
    "#  TEST DATA\n",
    "classifier = LogisticRegression_Classifier(trainData)\n",
    "predictions = predictLabels(test_Data, classifier)\n",
    "true_labels = list(map(lambda d: d[1], test_Data))\n",
    "a = accuracy_score(true_labels, predictions)\n",
    "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "print(\"accuracy: \", a)\n",
    "print(\"Precision: \", p)\n",
    "print(\"Recall: \", a)\n",
    "print(\"f1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "accuracy:  0.5700952380952381\n",
      "Precision:  0.5807250088739933\n",
      "Recall:  0.5700952380952381\n",
      "f1-score:  0.5600577879322785\n"
     ]
    }
   ],
   "source": [
    "#  TEST DATA\n",
    "classifier = SGDClassifier_Classifier(trainData)\n",
    "predictions = predictLabels(test_Data, classifier)\n",
    "true_labels = list(map(lambda d: d[1], test_Data))\n",
    "a = accuracy_score(true_labels, predictions)\n",
    "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
    "print(\"accuracy: \", a)\n",
    "print(\"Precision: \", p)\n",
    "print(\"Recall: \", a)\n",
    "print(\"f1-score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.878125\n"
     ]
    }
   ],
   "source": [
    "test_samples = testData\n",
    "pred = predictLabels(test_samples, classifier)\n",
    "summ = 0\n",
    "for i in range(len(test_samples)):\n",
    "    correct = np.where(test_samples[i][1] == pred[i], 1, 0)\n",
    "    # print(f'True: {test_samples[i][1]}, Predict: {pred[i]},   {correct}')\n",
    "    summ += correct\n",
    "\n",
    "print(f'Acc: {summ/len(test_samples)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the review': 1, 'review we': 1, 'we read': 1, 'read were': 1, 'were a': 1, 'a bit': 1, 'bit mixed': 1, 'mixed but': 1, 'but i': 1, 'i thought': 1, 'thought it': 1, 'it wa': 1, 'wa excellent': 1, 'excellent ive': 1, 'ive stayed': 1, 'stayed in': 1, 'in the': 1, 'the splendido': 1, 'splendido villa': 1, 'villa deste': 1, 'deste and': 1, 'and others': 1, 'others and': 1, 'and it': 1, 'it doesnt': 1, 'doesnt compare': 1, 'compare but': 1, 'but nor': 1, 'nor should': 1, 'should it': 1, 'it it': 1, 'doesnt claim': 1, 'claim to': 1, 'to be': 1, 'be in': 1, 'in that': 1, 'that space': 1, 'space i': 1, 'i expected': 1, 'expected it': 1, 'it to': 1, 'be a': 1, 'a notch': 1, 'notch off': 1, 'off the': 1, 'the four': 1, 'four season': 1, 'season and': 1, 'and so': 1, 'so it': 1, 'it proved': 1, 'proved a': 1, 'a only': 1, 'only a': 1, 'a small': 1, 'small notch': 1, 'notch too': 1, 'too lovely': 1, 'lovely clean': 1, 'clean and': 1, 'and bright': 1, 'bright room': 1, 'room superb': 1, 'superb view': 1, 'view very': 1, 'very quiet': 1, 'quiet comfy': 1, 'comfy bed': 1, 'bed etc': 1, 'etc excellent': 1, 'excellent service': 1, 'service all': 1, 'all round': 1, 'round and': 1, 'and nice': 1, 'nice staff': 1, 'staff for': 1, 'for the': 1, 'the money': 1, 'money and': 1, 'and quality': 1, 'quality it': 1, 'it stacked': 1, 'stacked up': 1, 'up very': 1, 'very well': 1, 'well indeed': 1, 'indeed couldnt': 1, 'couldnt really': 1, 'really fault': 1, 'fault it': 1, 'it any': 1, 'any way': 1, 'way given': 1, 'given it': 1, 'wa half': 1, 'half the': 1, 'the cost': 1, 'cost of': 1, 'of the': 1, 'season but': 1, 'but pretty': 1, 'pretty close': 1, 'close in': 1, 'in term': 1, 'term of': 1, 'of overall': 1, 'overall experience': 1, 'experience we': 1, 'we will': 1, 'will go': 1, 'go there': 1, 'there again': 1, 'again when': 1, 'when in': 1, 'in chicago': 1, 'chicago great': 1, 'great location': 1, 'location too': 1, 'too ': 1, 'the': 1, 'review': 1, 'we': 1, 'read': 1, 'were': 1, 'a': 1, 'bit': 1, 'mixed': 1, 'but': 1, 'i': 1, 'thought': 1, 'it': 1, 'wa': 1, 'excellent': 1, 'ive': 1, 'stayed': 1, 'in': 1, 'splendido': 1, 'villa': 1, 'deste': 1, 'and': 1, 'others': 1, 'doesnt': 1, 'compare': 1, 'nor': 1, 'should': 1, 'claim': 1, 'to': 1, 'be': 1, 'that': 1, 'space': 1, 'expected': 1, 'notch': 1, 'off': 1, 'four': 1, 'season': 1, 'so': 1, 'proved': 1, 'only': 1, 'small': 1, 'too': 1, 'lovely': 1, 'clean': 1, 'bright': 1, 'room': 1, 'superb': 1, 'view': 1, 'very': 1, 'quiet': 1, 'comfy': 1, 'bed': 1, 'etc': 1, 'service': 1, 'all': 1, 'round': 1, 'nice': 1, 'staff': 1, 'for': 1, 'money': 1, 'quality': 1, 'stacked': 1, 'up': 1, 'well': 1, 'indeed': 1, 'couldnt': 1, 'really': 1, 'fault': 1, 'any': 1, 'way': 1, 'given': 1, 'half': 1, 'cost': 1, 'of': 1, 'pretty': 1, 'close': 1, 'term': 1, 'overall': 1, 'experience': 1, 'will': 1, 'go': 1, 'there': 1, 'again': 1, 'when': 1, 'chicago': 1, 'great': 1, 'location': 1, '': 1} real\n"
     ]
    }
   ],
   "source": [
    "print(trainData[1][0], trainData[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainData[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bf0f833311fc1a2d9ca1cf8207d63bf4ae3af89d2c46669381eed313e768b9d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('SNLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
