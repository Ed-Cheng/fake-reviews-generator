{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils.loader import DataLoader\n",
    "import datasets\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import csv\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lem = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "loader = DataLoader()\n",
    "train_data = loader.load_amazon(deceptive=False, all=True, test_mode=False)\n",
    "test_data = loader.load_amazon(deceptive=False, all=True, test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(txt):\n",
    "    '''Basic text preprocessing, including \n",
    "    to lowercases, remove punctuations/odd characters/slash_n/meaningless words, tokenize, lemmatization'''\n",
    "    # convert to lowercases\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # replace punctuations with spaces\n",
    "    for punc in string.punctuation:\n",
    "        txt = txt.replace(punc, ' ')\n",
    "\n",
    "    # remove odd characters (keep alphabets only)\n",
    "    txt = re.sub(r'[^a-z ]', '', txt)\n",
    "\n",
    "    # tokenize the txt\n",
    "    txt = word_tokenize(txt)\n",
    "\n",
    "    # # lemmatization\n",
    "    # txt = [lem.lemmatize(word) for word in txt]\n",
    "\n",
    "    # # stop word removel, too-short word removel\n",
    "    # stop_words = stopwords.words('english')\n",
    "    # txt = [w for w in txt if w not in stop_words and len(w) > 1]\n",
    "\n",
    "    return txt\n",
    "\n",
    "\n",
    "def get_inv_idx(raw_txt):\n",
    "    '''calculate the inverted index and store with nested dictionaries:\n",
    "    a dictionary stores all the terms\n",
    "    each term contains a dictionary, stores all related doc and occurance\n",
    "    '''\n",
    "    print('Start calculating inverted index...')\n",
    "    inverted_idx = {}\n",
    "\n",
    "    # variables keep track of already processed passages\n",
    "    count_psg = 0\n",
    "\n",
    "    for i in range(len(raw_txt)):\n",
    "\n",
    "        passage = pre_process(raw_txt[i])\n",
    "\n",
    "        for w in passage:\n",
    "            if w in inverted_idx:\n",
    "                inverted_idx[w][i] = 1\n",
    "            else:\n",
    "                inverted_idx[w] = {}\n",
    "                inverted_idx[w][i] = 1\n",
    "        count_psg += 1\n",
    "    \n",
    "    print('Total processed passages:', count_psg)\n",
    "    print('Finish calculating inverted index')\n",
    "    return inverted_idx\n",
    "\n",
    "\n",
    "def cal_tf_idf(txt, N, inv_idx):\n",
    "    ''' calculate the tf-idf of a pre-processed text\n",
    "     Args:\n",
    "        txt:        pre-processed text\n",
    "        N:          number of documents in collection\n",
    "        inv_idx:    inverted index of terms    \n",
    "    '''\n",
    "    term_size = len(inv_idx)\n",
    "    key_list = list(inv_idx)\n",
    "    \n",
    "    tf_idf = np.zeros(term_size)\n",
    "    for t in np.unique(txt):\n",
    "        # number of documents in which term t appears\n",
    "        if t in inv_idx:\n",
    "            n_t = len(inv_idx[t])\n",
    "            idf = np.log10(N/n_t)\n",
    "            tf = txt.count(t)\n",
    "\n",
    "            term_idx = key_list.index(t)\n",
    "            tf_idf[term_idx] = tf * idf\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train_data['REVIEW_TEXT']\n",
    "text_test = test_data['REVIEW_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_data['LABEL']\n",
    "label_test = test_data['LABEL']\n",
    "\n",
    "lab = []\n",
    "for i in range(len(label)):\n",
    "    if label[i] == 0:\n",
    "        lab.append('fake')\n",
    "    else:\n",
    "        lab.append('true')\n",
    "\n",
    "labt = []\n",
    "for i in range(len(label)):\n",
    "    if label[i] == 0:\n",
    "        labt.append('fake')\n",
    "    else:\n",
    "        labt.append('true')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = []\n",
    "for i in range(len(text)):\n",
    "    tx.append((text[i]))\n",
    "\n",
    "for i in range(len(text_test)):\n",
    "    tx.append((text_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start calculating inverted index...\n",
      "Total processed passages: 21000\n",
      "Finish calculating inverted index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33075"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my own inverted idx\n",
    "inverted_index = get_inv_idx(tx)\n",
    "len(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process(tx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 21000\n",
    "tf_idf = []\n",
    "for i in range(N):\n",
    "    tf_idf.append(cal_tf_idf(pre_process(tx[i]), N, inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf_idf[:len(text)]\n",
    "y = tf_idf[len(text):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(tx)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vec = vectorizer.fit_transform(tx)\n",
    "# y = vectorizer.fit_transform(txtest)\n",
    "X = vec[:len(text)]\n",
    "y = vec[len(text):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15750, 34870) (5250, 34870)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Classifier \t 0.5095238095238095\n",
      "Perceptron \t \t 0.5022857142857143\n",
      "Passive-Aggressive \t 0.5051428571428571\n",
      "kNN \t \t \t 0.5038095238095238\n",
      "LinearSVC\t \t 0.5062857142857143\n",
      "SGDClassifier\t \t 0.5108571428571429\n",
      "MultinomialNB\t \t 0.5059047619047619\n",
      "BernoulliNB\t \t 0.496952380952381\n"
     ]
    }
   ],
   "source": [
    "for clf, name in (\n",
    "        (RidgeClassifier(), \"Ridge Classifier\"),\n",
    "        (Perceptron(), \"Perceptron \\t\"),\n",
    "        (PassiveAggressiveClassifier(), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(), \"kNN \\t \\t\"),\n",
    "        (LinearSVC(), \"LinearSVC\\t\"),\n",
    "        (SGDClassifier(), \"SGDClassifier\\t\"),\n",
    "        (MultinomialNB(), \"MultinomialNB\\t\"),\n",
    "        (BernoulliNB(), \"BernoulliNB\\t\")):\n",
    "\n",
    "\n",
    "    clf.fit(X, lab)\n",
    "\n",
    "    pred = clf.predict(y)\n",
    "\n",
    "    count = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == labt[i]:\n",
    "            count += 1\n",
    "\n",
    "    print(f'{name} \\t {count/len(pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fit\n",
      "start pred\n",
      "sgd \t 0.5055238095238095\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier()\n",
    "print(\"start fit\")\n",
    "\n",
    "clf.fit(X, lab)\n",
    "\n",
    "print(\"start pred\")\n",
    "pred = clf.predict(y)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == labt[i]:\n",
    "        count += 1\n",
    "\n",
    "print(f'sgd \\t {count/len(pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1bf0f833311fc1a2d9ca1cf8207d63bf4ae3af89d2c46669381eed313e768b9d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('SNLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
