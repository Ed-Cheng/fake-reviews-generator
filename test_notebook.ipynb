{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15750"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(DataLoader().load_amazon(test_mode=True, all=True)))\n",
    "len(DataLoader().load_amazon(all=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.25\n",
    "loader = DataLoader()\n",
    "truth_data = pd.read_table('data/amazon_reviews/amazon_reviews.txt')\n",
    "truth_data = truth_data.sample(frac=1) # shuffle\n",
    "truth_data_test = truth_data[:int(test_ratio * len(truth_data))]\n",
    "truth_data_train = truth_data[int(test_ratio * len(truth_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_data_train.to_csv('amazon_reviews.txt', index=False)\n",
    "truth_data_test.to_csv('amazon_reviews2.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.gpt import GPT2\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('checkpoints/distilgpt2/checkpoint-6336')\n",
    "gpt = GPT2(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['My favourite game of the year in the history of this game.  It feels as though the player is constantly working to improve the game and that was true of this game.  The game also feels less action and more character-driven as the player progresses.  You have enough time to be in the mood to really appreciate the game and the quality of each piece of dialogue being cut in an action.',\n",
       " 'My favourite, soft drink. This is a perfect blend for coffee and tea.  I really enjoy it.',\n",
       " 'My favourite chocolate cake I have tried but it was better.  It seems to be made from wood and has quite a bit of room, so it was nice to have it sitting in an area while making it.  We really enjoyed the texture and the simplicity of the cake.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.generate_text('My favourite', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8498b917358463ca6116f84823d083c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6b27ad23fd45dd8f3c0fd90cac2449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForCausalLM, TrainingArguments, \\\n",
    "\tTrainer, DataCollatorWithPadding\n",
    "import datasets\n",
    "import torch\n",
    "from utils.loader import DataLoader\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2', use_fast=True)\n",
    "val_ratio = 0.2\n",
    "block_size = 1024\n",
    "\n",
    "\n",
    "def df_to_dataset_obj(dataframe, columns):\n",
    "\tdataset = datasets.Dataset.from_pandas(dataframe[columns])\n",
    "\tdataset = dataset.remove_columns('__index_level_0__')\n",
    "\tdataset = dataset.rename_column('LABEL', 'labels')\n",
    "\tdataset = dataset.rename_column('REVIEW_TEXT', 'text')\n",
    "\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "def tokenize_data(inputs):\n",
    "\ttokens = tokenizer(inputs['text'], padding='max_length', truncation=True, max_length=block_size)\n",
    "\ttokens['labels'] = tokens['input_ids'].copy()\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Load datasets\n",
    "data_loader = DataLoader()\n",
    "truth_data = data_loader.load_amazon(deceptive=False)\n",
    "truth_data = truth_data.sample(frac=1)\n",
    "truth_data_val = truth_data.iloc[:int(val_ratio * len(truth_data))]\n",
    "truth_data_train = truth_data.iloc[int(val_ratio * len(truth_data)):]\n",
    "\n",
    "# Clean and convert to Dataset objects\n",
    "# dataset_dec = df_to_dataset_obj(dec_data, ['LABEL', 'REVIEW_TEXT'])\n",
    "dataset_truth_val = df_to_dataset_obj(truth_data_val, ['LABEL', 'REVIEW_TEXT'])\n",
    "dataset_truth_train = df_to_dataset_obj(truth_data_train, ['LABEL', 'REVIEW_TEXT'])\n",
    "\n",
    "# tokenized_dec = dataset_dec.map(tokenize_data(tokenizer=), batched=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_val = dataset_truth_val.map(tokenize_data, batched=True, remove_columns=['text'])\n",
    "tokenized_train = dataset_truth_train.map(tokenize_data, batched=True, remove_columns=['text'])\n",
    "\n",
    "lm_train = tokenized_train\n",
    "lm_val = tokenized_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 6333\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_train[11][\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "591ade058296000ef487104c42d37478df75296e039f312fe940d4accf2fd7b8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
