{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from utils.loader import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'distilgpt2'\n",
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",                    \n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "data_loader = DataLoader()\n",
    "truth_data = data_loader.load_amazon(test_mode=True, deceptive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenier(special_tokens=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True) #GPT2Tokenizer\n",
    "\n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        print(\"Special tokens added\")\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
    "    if special_tokens:\n",
    "        config = AutoConfig.from_pretrained(MODEL, \n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            sep_token_id=tokenizer.sep_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            output_hidden_states=False)\n",
    "    else: \n",
    "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            output_hidden_states=False)    \n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL, config=config)\n",
    "\n",
    "    if special_tokens:\n",
    "        #Special tokens added, model needs to be resized accordingly\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if load_model_path:\n",
    "        model.load_state_dict(torch.load(load_model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    #model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenier()\n",
    "model = get_model(tokenizer, 'training/gpt-gold/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data = data_loader.load_gold_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_start(df, length=5):\n",
    "    sample = df.sample(n=1)\n",
    "    text = list(sample['REVIEW_TEXT'])[0]\n",
    "    sample = str(text).split(' ')\n",
    "    return ' '.join(sample[:length]), text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, original = sample_start(gold_data, length=np.random.randint(5, 8))\n",
    "#prompt = SPECIAL_TOKENS['bos_token'] + cat + SPECIAL_TOKENS['sep_token'] + prompt\n",
    "\n",
    "tokenized_prompt = tokenizer.encode('Here is my review: ' + prompt, return_tensors='pt')\n",
    "out = model.generate(tokenized_prompt, max_length=50, top_k=50,  top_p=0.7, num_return_sequences=3, do_sample=True, early_stopping=True, repition_penalty=1.5)\n",
    "print(f'Text: {original[:300]}...\\n')\n",
    "for i, o in enumerate(out):\n",
    "    gen_txt = tokenizer.decode(o, skip_special_tokens=True)\n",
    "    print(gen_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenier(SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, load_model_path='training/distilgpt-topic2/pytorch_model.bin', special_tokens=SPECIAL_TOKENS)\n",
    "#model = AutoModelForCausalLM.from_pretrained('training/distilgpt-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_start(df, length=5):\n",
    "    sample = df.sample(n=1)\n",
    "    title, category, text = list(sample['REVIEW_TITLE'])[0], list(sample['PRODUCT_CATEGORY'])[0], list(sample['REVIEW_TEXT'])[0]\n",
    "    sample = str(text).split(' ')\n",
    "    return ' '.join(sample[:length]), title, category, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This is a handy holder/dispenser for wipes. The opening is large enough that it's easy to insert the wipes. Although the handle would be useful for a stroller, I like to put mine in the glove compartm \n",
      "Prompt: I love this....\n",
      "\n",
      "I love this.  I have been using it for over a year now and am very pleased with the quality of the product.\n",
      "\n",
      "I love this.  I have been using it for a couple of years now and am very happy with it.\n",
      "\n",
      "I love this.  It is a little small and easy to put together.  I am very happy with it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No cat\n",
    "prompt, title, cat, original = sample_start(truth_data, length=np.random.randint(5, 8))\n",
    "#prompt = SPECIAL_TOKENS['bos_token'] + cat + SPECIAL_TOKENS['sep_token'] + prompt\n",
    "prompt = 'I love this.'\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors='pt')\n",
    "out = model.generate(tokenized_prompt, do_sample=True, max_length=70, num_beams=5, repetition_penalty=5.0, early_stopping=True, num_return_sequences=3)\n",
    "print(f'Text: {original[:200]} \\nPrompt: {prompt}...\\n')\n",
    "for i, o in enumerate(out):\n",
    "    gen_txt = tokenizer.decode(o, skip_special_tokens=True)\n",
    "    truncated_txt = gen_txt.split('.')\n",
    "    print('.'.join(truncated_txt[:-1]) + '.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Books  \n",
      "Prompt: <|BOS|>Books<|SEP|>I love this....\n",
      "\n",
      "I love this.  It's the best book I've ever read.  The author is a true storyteller and has done everything he can to make my life so much easier.\n",
      "\n",
      "I love this.  I read a lot of books on how to make your life so much easier.<br /><br />This book is not for the faint of heart.\n",
      "\n",
      "I love this.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With cat\n",
    "prompt, title, cat, original = sample_start(truth_data, length=np.random.randint(5, 8))\n",
    "cat = 'Books'\n",
    "prompt = 'I love this.'\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + cat + SPECIAL_TOKENS['sep_token'] + prompt\n",
    "pos = len(cat)\n",
    "\n",
    "tokenized_prompt = tokenizer.encode(prompt, return_tensors='pt')\n",
    "#out = model.generate(tokenized_prompt, max_length=50, top_k=50,  top_p=0.7, num_return_sequences=3, do_sample=True, repetition_penalty=1)\n",
    "out = model.generate(tokenized_prompt, do_sample=True, max_length=70, num_beams=5, repetition_penalty=5.0, early_stopping=True, num_return_sequences=3)\n",
    "print(f'Category: {cat}  \\nPrompt: {prompt}...\\n')\n",
    "for i, o in enumerate(out):\n",
    "    gen_txt = tokenizer.decode(o, skip_special_tokens=True)\n",
    "    truncated_txt = gen_txt[pos:].split('.')\n",
    "    print('.'.join(truncated_txt[:-1]) + '.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8498b917358463ca6116f84823d083c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6b27ad23fd45dd8f3c0fd90cac2449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, AutoModelForCausalLM, TrainingArguments, \\\n",
    "\tTrainer, DataCollatorWithPadding\n",
    "import datasets\n",
    "import torch\n",
    "from utils.loader import DataLoader\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2', use_fast=True)\n",
    "val_ratio = 0.2\n",
    "block_size = 1024\n",
    "\n",
    "\n",
    "def df_to_dataset_obj(dataframe, columns):\n",
    "\tdataset = datasets.Dataset.from_pandas(dataframe[columns])\n",
    "\tdataset = dataset.remove_columns('__index_level_0__')\n",
    "\tdataset = dataset.rename_column('LABEL', 'labels')\n",
    "\tdataset = dataset.rename_column('REVIEW_TEXT', 'text')\n",
    "\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "def tokenize_data(inputs):\n",
    "\ttokens = tokenizer(inputs['text'], padding='max_length', truncation=True, max_length=block_size)\n",
    "\ttokens['labels'] = tokens['input_ids'].copy()\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Load datasets\n",
    "data_loader = DataLoader()\n",
    "truth_data = data_loader.load_amazon(deceptive=False)\n",
    "truth_data = truth_data.sample(frac=1)\n",
    "truth_data_val = truth_data.iloc[:int(val_ratio * len(truth_data))]\n",
    "truth_data_train = truth_data.iloc[int(val_ratio * len(truth_data)):]\n",
    "\n",
    "# Clean and convert to Dataset objects\n",
    "# dataset_dec = df_to_dataset_obj(dec_data, ['LABEL', 'REVIEW_TEXT'])\n",
    "dataset_truth_val = df_to_dataset_obj(truth_data_val, ['LABEL', 'REVIEW_TEXT'])\n",
    "dataset_truth_train = df_to_dataset_obj(truth_data_train, ['LABEL', 'REVIEW_TEXT'])\n",
    "\n",
    "# tokenized_dec = dataset_dec.map(tokenize_data(tokenizer=), batched=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_val = dataset_truth_val.map(tokenize_data, batched=True, remove_columns=['text'])\n",
    "tokenized_train = dataset_truth_train.map(tokenize_data, batched=True, remove_columns=['text'])\n",
    "\n",
    "lm_train = tokenized_train\n",
    "lm_val = tokenized_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 6333\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_train[11][\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "591ade058296000ef487104c42d37478df75296e039f312fe940d4accf2fd7b8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
