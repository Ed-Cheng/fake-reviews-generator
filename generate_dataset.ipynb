{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loader import DataLoader\n",
    "from models.gpt import GPT2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",                    \n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_start_amazon(df, length=5):\n",
    "    sample = df.sample(n=1)\n",
    "    title, category, text = list(sample['REVIEW_TITLE'])[0], list(sample['PRODUCT_CATEGORY'])[0], list(sample['REVIEW_TEXT'])[0]\n",
    "    sample = str(text).split(' ')\n",
    "    return ' '.join(sample[:length]), title, category, text\n",
    "\n",
    "def sample_start_gold(df, length=5):\n",
    "    sample = df.sample(n=1)\n",
    "    text = list(sample['REVIEW_TEXT'])[0]\n",
    "    sample = str(text).split(' ')\n",
    "    return ' '.join(sample[:length]), text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mazab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load our test-data that we will be sampling categories and prompts from\n",
    "data_loader = DataLoader()\n",
    "data_amazon = data_loader.load_amazon(test_mode=True, deceptive=False)\n",
    "data_gold = data_loader.load_gold_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model\n",
    "model_path = 'training/distilgpt-topic2/pytorch_model.bin'\n",
    "model = GPT2(model_path=model_path, full_model=False, special_tokens=SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the available categories\n",
    "categories = ['Apparel', 'Automotive', 'Baby', 'Beauty', 'Books', 'Camera', 'Electronics', 'Furniture', 'Grocery', 'Health & Personal Care', 'Home', 'Home Entertainment', 'Home Improvement', 'Jewelry', 'Kitchen', 'Lawn and Garden', 'Luggage', 'Musical Instruments', 'Office Products', 'Outdoors', 'PC', 'Pet Products', 'Shoes', 'Sports', 'Tools', 'Toys', 'Video DVD', 'Video Games', 'Watches', 'Wireless']\n",
    "start_words = ['A', 'The', 'We', 'I', 'This', 'I love', 'I hate', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin generating samples\n",
    "# 25k will be sampled with random category and random start word from OPSpam as it has better grammar\n",
    "# 25k sampled from Amazon dataset with corresponding category and first 2-5 words and let GPT finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we sample a random category, and a random start word.\n",
    "all_reviews = []\n",
    "while len(all_reviews) < 5000:\n",
    "    print(f'{len(all_reviews)}/25000')\n",
    "    prompt, original = sample_start_gold(data_gold, length=1)\n",
    "    cat = np.random.choice(categories)\n",
    "    prompt = SPECIAL_TOKENS['bos_token'] + cat + SPECIAL_TOKENS['sep_token'] + prompt\n",
    "    outputs = model.generate_text(prompt, cat, print_output=False, do_sample=True, max_length=200, num_beams=5, repetition_penalty=5.0, early_stopping=True, num_return_sequences=3)\n",
    "    for review in outputs:\n",
    "        if len(review) > 10: # Ensure text generated is text\n",
    "            all_reviews.append([cat, review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we sample a random review and use its category and a random length start of sentence prompt.\n",
    "all_reviews = []\n",
    "while len(all_reviews) < 5000:\n",
    "    print(f'{len(all_reviews)}/25000')\n",
    "    # Sample a random prompt and corresponding category from the dataset and gemerate\n",
    "    prompt, title, cat, original = sample_start_amazon(data_amazon, length=np.random.randint(2, 5))\n",
    "    prompt = SPECIAL_TOKENS['bos_token'] + cat + SPECIAL_TOKENS['sep_token'] + prompt\n",
    "    outputs = model.generate_text(prompt, cat, print_output=False, do_sample=True, max_length=70, num_beams=5, repetition_penalty=5.0, early_stopping=True, num_return_sequences=3)\n",
    "    for review in outputs:\n",
    "        if len(review) > 10: # Ensure text generated is text\n",
    "            all_reviews.append([cat, review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews = pd.DataFrame(all_reviews, columns=['PRODUCT_CATEGORY', 'REVIEW_TEXT'])\n",
    "fake_reviews = fake_reviews.drop_duplicates('REVIEW_TEXT')\n",
    "fake_reviews = fake_reviews.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews_second = pd.DataFrame(all_reviews, columns=['PRODUCT_CATEGORY', 'REVIEW_TEXT'])\n",
    "fake_reviews_second = fake_reviews_second.drop_duplicates('REVIEW_TEXT')\n",
    "fake_reviews_second = fake_reviews_second.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_reviews['SAMPLE_TYPE'] = 'NON-GUIDED'\n",
    "fake_reviews_second['SAMPLE_TYPE'] = 'GUIDED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([fake_reviews, fake_reviews_second])\n",
    "final_data = final_data.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['REVIEW_TEXT'] = final_data['REVIEW_TEXT'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('gpt_generated_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13872f827bf2face4951d508a343680c0c465a86f8c76a51d647b255bdadb53b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
